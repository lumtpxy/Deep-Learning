## 参数的更新

神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为**最优化**

为了找到最优参数，我们将参数的梯度（导数）作为了线索。使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为**随机梯度下降法**，简称SGD。

### 探险家的故事

寻找最优参数时，我们所处的状况和探险家一样，是一个漆黑的世界。我们必须在没有地图、不能睁眼的情况下，在广袤、复杂的地形中寻找“至深之地”

### SGD

用数学式可以将SGD写成：
```math
W \gets W - \eta \frac{\partial_L}{\partial_W}
```
这里把需要更新的权重参数记为W，把损失函数关于W的梯度记为 $\frac{\partial_L}{\partial_W}$。$\eta$ 表示学习率。式中的 $\gets$ 表示用右边的值更新左边的值

### SGD的缺点

虽然SGD简单，并且容易实现，但是在解决某些问题时可能没有效率

求下面函数的最小值问题：
```math
f(x, y)=
\frac{1}{20}x^2 + y^2
```
该函数是向x轴方向延伸的“碗”状函数。它的等高线呈向x轴方向延伸的椭圆状

对该函数应用SGD，SGD呈“之”字形移动。这是一个相当低效的路径。
也就是说，SGD的缺点是，如果函数的形状非均匀向，比如呈延伸状，搜索的路径就会非常低效
SGD低效的根本原因是，梯度的方向并没有指向最小值的方向

### Momentum

Momentum 是“动量”的意思，和物理有关。用数学式表示：
```math
\upsilon \gets \alpha \upsilon - \eta \frac{\partial_L}{\partial_W}\\
W \gets W + \upsilon
```
这里新出现了一个变量 $\upsilon$，对应物理上的速度
上式表示了物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则

现在尝试使用 Momentum 解决上个函数最优化问题。
更新路径就像小球在碗中滚动一样。和SGD相比，我们发现“之”字形的“程度”减轻了
这是因为虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会相互抵消，所以y轴方向上的速度不稳定
因此，和SGD时的情形相比，可以更快地朝x轴方向靠近，减轻“之”字形的变动程度

### AdaGrad

在神经网络的学习中，学习率的值很重要。学习率过小，会导致学习花费过多的时间；反过来，学习率过大，则会导致学习发散而不能正确进行

在关于学习率的有效技巧中，有一种被称为**学习率衰减**的方法，即随着学习的进行，使学习率逐渐减小

AdaGrad 会为参数的每个元素适当地调整学习率，与此同时进行学习（AdaGrad 的 Ada 来自英文单词 Adaptive，即“适当的”的意思）

用数学式表达：
```math
h \gets h + \frac{\partial_L}{\partial_W} \odot \frac{\partial_L}{\partial_W}\\
```
```math
W \gets W - \eta \frac{1}{\sqrt{h}} \frac{\partial_L}{\partial_W}
```
这里新出现了变量h，它保存了以前的所有梯度值的平方和（$\odot$ 表示对应矩阵元素的乘法）

> :pushpin:
> AdaGrad 会记录过去所有梯度的平方和。
> 因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为0，完全不再更新
> 为了改善这个问题，可以使用 RMSPorp 方法。RMSPorp 方法并不是将过去所有梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来

基于 AdaGrad 的最优化个更新路径，函数的取值高效地向着最小值移动。由于y轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此，y轴方向上的更新程度被减弱，“之”字形的变动程度有所衰减

### Adam

Momentum 参照小球在碗中滚动的物理规则进行移动，AdaGrad 为参数的每个元素适当地调整更新步伐。如果将这两个方法融合在一起会怎么样呢？这就是 Adam 方法的基本思路

基于 Adam 的更新过程就像小球在碗中滚动一样。虽然 Momentum 也有类似的移动，但是相比之下，Adam 的小球左右摇晃的程度有所减轻。这得益于学习的更新程度被适当地调整了

### 使用哪种更新方法呢

非常遗憾，（目前）并不存在能在所有问题中都表现良好的方法。这4中方法各有各的特点，都有各自擅长解决的问题和不擅长解决的问题

## 权重的初始值

在神经网络的学习中，权重的初始值特别重要。实际上，设定什么样的权重初始值，经常关系到神经网络的学习能否成功

### 可以将权重初始值设为0吗

为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始值设成一样的值呢？
这是因为在误差反向传播法中，所有的权重值都会进行相同的更新。权重被更新为相同的值，并拥有了对称的值（重复的值）。这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化”（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值

### 隐藏层的激活值分布

观察隐藏层的激活值（激活函数的输出数据）的分布，可以获得很多启发

向一个5层神经网络（激活函数使用 sigmoid 函数）传入随机生成的输入数据，用直方图绘制各层激活值的数据分布

使用标准差为1的高斯分布作为权重初始值时：
这里使用的 sigmoid 函数是 S 型函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0。因此，偏向0和1的数据分布会造成反向传播中梯度的值不断减小，最后消失。这个问题称为**梯度消失**

使用标准差为0.01的高斯分布作为权重初始值时：
这次呈集中在0.5附近的分布。因为不像刚才的例子那样偏向0和1，所以不会发生梯度消失的问题
但是，激活值的分布有所偏向，说明在表现力上会有很大问题。为什么这么说呢？因为如果有多个神经元都输出几乎相同的值，那它们就没有存在的意思了。
比如，如果100个神经元都输出几乎相同的值，那么也可以由1个神经元来表达基本相同的事情。
因此，激活值在分布上有所偏向会出现“表现力受限”的问题

## Batch Normalization

为了使各层拥有适当的广度，“强制性”地调整激活值的分布会怎么样呢？实际上，Batch Normalization 方法就是基于这个想法而产生的

为什么 Batch Normalization 这么惹人注目呢？
- 可以使学习快速进行（可以增大学习率）
- 不那么依赖初始值（对于初始值不用那么神经质）
- 抑制过拟合（降低Dropout等的必要性）

Batch Normalization 的思路是调整各层的激活值分布使其拥有适当的广度。为此，要向神经网络中插入对数据分布进行正规化的层，即 Batch Normalization 层

## 正则化

机器学习的问题中，**过拟合**是一个很常见的问题。过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。
机器学习的目标是提高泛化能力，即便是没有包含在训练数据里的未观测数据，也希望模型可以正确的识别。

### 过拟合

发生过拟合的原因：
- 模型拥有大量参数、表现力强
- 训练数据少

### 权值衰减

**权值衰减**是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的

### Dropout

如果网络的模型变得很复杂，只用权值衰减就难以应对了。在这种情况下，使用 Dropout 方法
Dropout 是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递。

## 超参数的验证

神经网络中，除了权重和偏置等参数，**超参数**也经常出现。这里所说的超参数是指，比如各层的神经元的数量、batch大小、参数更新时的学习率或权重衰减等

### 验证数据

下面我们要对超参数设置各种各样的值以进行验证。这里要注意的是，不能使用测试数据评估超参数的性能

为什么不能使用测试数据评估超参数的性能呢？
这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏“，就会导致超参数的值被调整为只拟合测试数据。这样的话，可能就会得到不能拟合其他数据、泛化能力低的模型。

因此，调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为**验证数据**。我们使用这个验证数据来评估超参数的好坏

### 超参数的最优化

**步骤0**
设定超参数的范围

**步骤1**
从设定的超参数范围中随机采样

**步骤2**
使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置得很小）

**步骤3**
重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围

在超参数的最优化中，要注意的是深度学习需要很长时间（比如，几天或几周）。因此，在超参数的搜索中，需要尽早放弃那些不符合逻辑的超参数

-------

## 本章所学的内容

1. 参数的更新方法，除了SGD之外，还有Momentum、AdaGrad、Adam等方法
2. 权重初始值的赋值方法对进行正确的学习非常重要
3. 作为权重初始值，Xavier初始值、He初始值等比较有效
4. 通过使用Batch Normalization，可以加速学习，并且对初始值变得健壮
5. 抑制过拟合的正则化技术有权值衰减、Dropout等
6. 逐渐缩小”好值“存在的范围是搜索超参数的一个有效方法
