## 从数据中学习

神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指可以由数据自动决定权重参数的值。这是非常了不起的事情！因为如果所有的参数都需要人工决定的话，工作量就太大了。

### 数据驱动

数据是机器学习的命根子。从数据中寻找答案、从数据中发现模式、根据数据讲故事······这些机器学习所做的事情，如果没有数据的话，就无从谈起。因此，数据是机器学习的核心。这种数据驱动的方法，也可以说脱离了过往以人为中心的方法。

通过要解决某个问题，特别是需要发现某种模式时，人们一般会综合考虑各种因素后再给出答案。“这个问题好像有这样的规律性？”“不对，可能原因在别的地方。”——类似这样，人们以自己的经验和直觉为线索，通过反复试验推进工作。
而机器学习的方法则极力避免人为介入，尝试从收集到的数据中发现答案（模式）。神经网络或深度学习则比以往的机器学习方法更能避免人为介入。

现在我们来思考一个具体的问题，比如如何实现数字“5”的识别？
与其绞尽脑汁，从零开始想出一个可以识别5的算法，不如考虑通过有效利用数据来解决这个问题。
一种方案是，先从图像中提取**特征量**，再用机器学习技术学习这些特征量的模式。这里所说的“特征量”是指可以从输入数据中准确地提取本质数据（重要的数据）的转换器。

机器学习的方法中，由机器从收集到的数据中找出规律性。与从零开始想出算法相比，这种方法可以更高效地解决问题，也能减轻人的负担。
但是需要注意的是，将图像转换为向量时使用的特征量仍是由人设计的。对于不同的问题，必须使用合适的特征量（必须设计专门的特征量），才能得到好的结果。

> :pushpin:
> 深度学习有时也称为端到端机器学习。这里所说的**端对端**是指从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思。

神经网络的优点是对所有的问题都可以用同样的流程来解决。比如，不管要求解的问题是识别5，还是识别狗，抑或是识别人脸，神经网络都是通过不断地学习所提供的数据，尝试发现待求解的问题的模式。
也就是说，与待处理的问题无关，神经网络可以将数据直接作为原始数据，进行“端对端”的学习。

### 训练数据和测试数据

机器学习中，一般将数据分为**训练数据**和**测试数据**两部分来进行学习和实验等。
首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。

为什么需要将数据分为训练数据和测试数据呢？
因为我们追求的是模型的泛化能力。为了正确评价模型的**泛化能力**，就必须划分训练数据和测试数据。另外，训练数据也称为**监督数据**。

泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。

仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。
顺便说一下，只对某个数据集过度拟合的状态称为**过拟合**。避免过拟合也是机器学习的一个重要课题。

## 损失函数

神经网络的学习通过某个指标表示现在的状态，然后，以这个指标为基准，寻找最优权重参数。神经网络以某个指标为线索寻找最优权重参数。
神经网络的学习中所用的指标称为**损失函数**。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。

> :pushpin:
> 损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致
> 以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”
> 并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的
> 不管是用“恶劣程度”还是“优良程度”，做的事情本质上都是一样的

### 均方误差

可以用作损失函数的函数有很多，其中最有名的是**均方误差**：
```math
E=
\frac{1}{2}\displaystyle\sum_k (y_k-t_k)^2
```
这里，$y_k$表示神经网络的输出，$t_k$表示监督数据，k表示数据的维度。

### 交叉熵误差

除了均方误差之外，**交叉熵误差**也经常被用作损失函数：
```math
E=
-\displaystyle\sum_k t_k \log y_k
```
$t_k$是正确解标签，交叉熵误差的值是由正确解标签所对应的输出结果决定的

### mini-batch 学习

我们从全部数据中选出一部分，作为全部数据的“近似”。神经网络的学习也是从训练数据中选出一批数据，然后对每个 mini-batch 进行学习。比如，从 60000 个训练数据中随机选择 100 笔，再用这 100 笔数据进行学习。这种学习方式称为**mini-batch 学习**。

> :pushpin:
> mini-batch 的损失函数也是利用一部分样本数据来近似地计算整体
> 也就是说，用随机选择的小批量数据作为全体训练数据的近似值

### 为何要设定损失函数

既然我们的目标是获得识别精度尽可能高的神经网络，那不是应该把识别精度作为指标吗？

对于这一疑问，我们可以根据“导数”在神经网络学习中的作用来回答。在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。
为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值。

在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0。

## 数值微分

梯度法使用梯度的信息决定前进的方向

利用微小的差分求导数的过程称为**数值微分**

导数就是表示某个瞬间的变化量

有多个变量的函数的导数称为**偏导数**

由全部变量的偏导数汇总而成的向量称为**梯度**

梯度指示的方向是各点处的函数值减小最多的方向

### 梯度法

机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必须在学习时找到最优参数（权重和偏置）。这里所说的最优参数是指损失函数取最小值时的参数。
但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。

> :pushpin:
> 函数的极小值、最小值以及被称为鞍点的地方，梯度为0
> 极小值是局部最小值，也就是限定在某个范围内的最小值
> 鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点
> 虽然梯度法是要寻找梯度为0的地方，但是那个地方不一定就是最小值（也有可能是极小值或者鞍点）
> 此外，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期

在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进
像这样，通过不断地沿梯度方向前进，逐步减小函数值的过程就是**梯度法**

> :pushpin:
> 根据目的是寻找最小值还是最大值，梯度法的叫法有所不同。严格地讲，寻找最小值的梯度法称为梯度下降法，寻找最大值的梯度法称为梯度上升法
> 但是通过反转损失函数的符号，求最小值的问题和求最大值的问题会变成相同的问题，因此“下降”还是“上升”的差异本质上并不重要
> 一般来说，神经网络（深度学习）中，梯度法主要是指梯度下降法

在神经网络中，**学习率**决定在一次学习中，应该学习多少，以及在多大程度上更新参数。
学习率需要事先确定为某个值，比如0.01或0.001。一般而言，这个值过大或过小，都无法抵达一个“好的位置”。在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了

> :pushpin:
> 像学习率这样的参数称为**超参数**。这是一种和神经网络的参数（权重和偏置）性质不同的参数
> 相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的
> 一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定

### 神经网络的梯度

神经网络的学习也要求梯度。这里所说的梯度是指损失函数关于权重参数的梯度

## 学习算法的实现

神经网络的学习步骤如下：

**前提**

神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分成下面4个步骤

**步骤1(mini-batch)**

从训练数据中随机选出一部分数据，这部分数据称为 mini-batch。我们的目标是减小 mini-batch 的损失函数的值

**步骤2（计算梯度）**

为了减小 mini-batch 的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向

**步骤3（更新参数）**

将权重参数沿梯度方向进行微小更新

**步骤4（重复）**

重复步骤1、步骤2、步骤3

### 基于测试数据的评价

神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数据，即确认是否会发生过拟合
神经网络学习的最初目标是掌握泛化能力，因此，要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据

-------

## 本章所学的内容

1. 机器学习中使用的数据集分为训练数据和测试数据
2. 神经网络用训练数据进行学习，并用测试数据评价学习到的模型的泛化能力
3. 神经网络的学习以损失函数为指标，更新权重参数，以使损失函数的值减小
4. 利用某个给定的微小值的差分求导数的过程，称为数值微分
5. 利用数值微分，可以计算权重参数的梯度
