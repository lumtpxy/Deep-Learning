感知机在设定权重的工作，即确定合适的、能符合预期的输入和输出的权重，由人工进行的。

神经网络的出现就是为了解决这个。神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。

## 从感知机到神经网络

感知机用数学式来表示：
```math
y=
\begin{cases}
0(w1x1 + w2x2 <= \theta),\\
1(w1x1 + w2x2 > \theta)
\end{cases}
```
将上式改写成更加简洁的形式：

```math
y = h(b + w1x1 + w2x2)\\
h(x)=
\begin{cases}
0(x <= 0),\\
1(x > 0)
\end{cases}
```
输入信号的总和会被函数h(x)转换，转换后的值就是输出y。在输入超过0时返回1，否则返回0

h(x)函数会将输入信号的总和转换为输出信号，这种函数一般称为**激活函数**。如“激活”一词所示，激活函数的作用在于决定如何来激活输入信号的总和。

### 激活函数

以阈值为界，一旦输入超过阈值，就切换输出。这样的函数称为“阶跃函数”。因此，可以说感知机中使用了阶跃函数作为激活函数。

那么，如果感知机使用其他函数作为激活函数的话会怎么样呢？实际上，如果将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界了。

#### sigmoid 函数

```math
h(x)=\frac1{1+exp(-x)}
```
表示的 sigmoid 函数看上去有些复杂，但它也仅仅是个函数而已。而函数就是给定某个输入后，会返回某个输出的转换器

神经网络中用 sigmoid 函数作为激活函数，进行信号的转换，转换后的信号被传送给下一个神经元

#### sigmoid 函数和阶跃函数的比较

1. sigmoid 函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化
2. 阶跃函数只能返回0或1，sigmoid 函数可以返回0.3234、...、0.999等实数
3. 从宏观视角看，它们具有相似的形状
4. 不管输入信号有多小，或者有多大，输出信号的值都在0到1之间

#### 非线性函数

sigmoid 函数和阶跃函数还有其他共同点，就是两者均为非线性函数

> :pushpin:
> 函数本来是输入某个值后会返回一个值的转换器
> 向这个转换器输入某个值后，输出值是输入值的常数倍的函数称为线性函数（用数学式表示为$h(x)=cx$。c为常数）。因此，线性函数是一条笔直的直线
> 而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数

神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神经网络的层数就没有意义了。线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”

#### ReLU 函数

ReLU 函数可以表示为
```math
h(x)=
\begin{cases}
x(x > 0),\\
0(x <= 0)
\end{cases}
```

### 多维数组的运算

简单地讲，多维数组就是“数字的集合”，数字排成一列的集合、排成长方形的集合、排成三维状或者更一般化的N维状的集合

**在矩阵的乘积运算中，对应维度的元素个数要保持一致**

## 3层神经网络的实现

> :pushpin:
> 神经网络的运算可以作为矩阵运算打包进行

> :pushpin:
> 输出层所用的激活函数，要根据求解问题的性质决定
> 回归问题可以使用恒等函数，恒等函数会将输入按原样输出
> 二元分类问题可以使用 sigmoid 函数
> 多元分类问题可以使用 softmax 函数

> :pushpin:
> 机器学习的问题大致可以分为分类问题和回归问题
> 分类问题是数据属于哪一类别的问题
> 回归问题是根据某个输入预测一个（连续的）数值的问题

### softmax 函数的特征

softmax 函数的输出是0.0到1.0之间的实数。并且，softmax 函数的输出值的总和是1。输出总和为1是 softmax 函数的一个重要性质。正因为有了这个性质，我们才可以把 softmax 函数的输出解释为“概率”

一般而言，神经网络只把输出最大的神经元所对应的类别作为识别结果。并且，即使使用 softmax 函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的 softmax 函数可以省略。在实际问题中，由于指数函数的运算需要一定的计算机运算量，因此输出层的 softmax 函数一般会被省略。

### 输出层的神经元数量

输出层的神经元数量需要根据待解决的问题来决定。对于分类问题，输出层的神经元数量一般设定为类别的数量。

> :pushpin:
> 求解机器学习问题的步骤可以分为“学习”和“推理”两个阶段。首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。

> :pushpin:
> 使用神经网络解决问题时，也需要首先使用训练数据（学习数据）进行权重参数的学习；进行推理时，使用刚才学习到的参数，对输入数据进行分类。

> :pushpin:
> 对神经网络的输入数据进行某种既定的转换称为预处理
> 预处理在神经网络（深度学习）中非常实用，其有效性已在提高识别性能和学习的效率等众多实验中得到证明。

> :pushpin:
> 批处理对计算机的运算大有利处，可以大幅缩短每张图像的处理时间。那么为什么批处理可以缩短处理时间呢？这是因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化
> 并且，在神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数据总线的负荷（严格地讲，相当于数据读入，可以将更多的时间用在计算上）
> 也就是说，批处理一次性计算大型数组要比分开逐步计算各个小型数组速度更快

-------

## 本章所学的内容

1. 神经网络中的激活函数使用平滑变化的 sigmoid 函数或 ReLU 函数
2. 通过巧妙地使用 NumPy 多维数组，可以高效地实现神经网络
3. 机器学习的问题大体上可以分为回归问题和分类问题
4. 关于输出层的激活函数，回归问题中一般用恒等函数，分类问题中一般用 softmax 函数
5. 分类问题中，输出层的神经元的数量设置为要分类的类别数
6. 输入数据的集合称为批。通过以批为单位进行推理处理，能够实现高速的运算
