本章我们将学习一个能够高效计算权重参数的梯度的方法——误差反向传播法

要正确理解误差反向传播法，我个人认为有两种方法：一种是基于数学式；另一种是基于**计算图**

## 计算图

计算图将计算过程用图形表示出来。这里说的图形是数据结构图，通过多个节点和边表示（连接节点的直线称为“边”）。

### 用计算图求解

计算图通过节点和箭头表示计算过程。将计算的中间结果写在箭头的上方，表示各个节点的计算结果从左向右传递
构建了计算图后，从左向右进行计算。就像电路中的电流流动一样，计算结果从左向右传递。到达最右边的计算结果后，计算过程就结束了

用计算图解题的情况下，需要按照如下流程进行：
1. 构建计算图
2. 在计算图上，从左向右进行计算

这里的第2步“从左向右进行计算”是一种正方向上的传播，简称为**正向传播**。正向传播是从计算图出发点到结束点的传播。

### 局部计算

计算图的特征是可以通过传递“局部计算”获得最终结果。“局部”这个词的意思是“与自己相关的某个小范围”。局部计算是指，无论全局发生什么，都能只根据与自己相关的信息输出接下来的结果

计算图可以集中精力于局部计算。无论全局的计算有多么复杂，各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果

### 为何用计算图解题

那么计算图到底有什么优点呢？
1. 无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题
2. 利用计算图可以将中间的计算结果全部保存起来
3. 实际上，使用计算图最大的原因是，可以通过反向传播高效计算导数

## 链式法则

介绍链式法则时，需要先从**复合函数**说起，复合函数是由多个函数构成的函数。比如$z=(x+y)^2$是由下两个式子构成：
```math
z=t^2\\
t=x+y
```
链式法则是关于复合函数的导数的性质，定义如下：
如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示

### 链式法则和计算图

将链式法则的计算用计算图表示出来：沿着与正方向相反的方向，乘上局部导数后传递

最左边的是反向传播的结果。根据链式法则，$\frac{\partial_z}{\partial_z}\frac{\partial_z}{\partial_t}\frac{\partial_t}{\partial_x}=\frac{\partial_z}{\partial_t}\frac{\partial_t}{\partial_x}=\frac{\partial_z}{\partial_x}$成立
，对应“z关于x的导数”。也就是说，反向传播是基于链式法则的

## 反向传播

### 加法节点的反向传播

加法节点的反向传播将上游的值原封不动地输出到下游

### 乘法节点的反向传播

乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。
翻转值表示一种翻转关系，正向传播时信号是x的话，反向传播时则是y；正向传播时信号是y的话，反向传播时则是x

加法的反向传播只是将上游的值传给下游，并不需要正向传播的输入信号。
但是，乘法的反向传播需要正向传播时的输入信号值。因此，实现乘法节点的反向传播时，要保存正向传播的输入信号

## 简单层的实现

这里，我们把要实现的计算图的乘法节点称为“乘法层”，加法节点称为“加法层”

> :pushpin:
> 这里所说的“层”是神经网络中功能的单位
> 比如，负责 sigmoid 函数的 Sigmoid、负责矩阵乘积的 Affine 等，都以层为单位进行实现

## 激活函数层的实现

### ReLU 层

求激活函数 ReLU y 关于 x 的导数：
```math
\frac{\partial_y}{\partial_x}=
\begin{cases}
1(x > 0),\\
0(x <= 0)
\end{cases}
```

> :pushpin:
> ReLU 层的作用就像电路中的开关一样。
> 正向传播时，有电流通过的话，就将开关设为 ON，没有电流通过的话，就将开关设为 OFF
> 反向传播时，开关为 ON 的话，电流会直接通过，开关为 OFF 的话，则不会有电流通过

### Sigmoid 层

```math
\text{'/'节点:}
\quad\quad
\frac{\partial_y}{\partial_x}=-\frac{1}{x^2}=-y^2\\

\text{'exp'节点:}
\quad\quad
\frac{\partial_y}{\partial_x}=exp(x)\\

\text{最左边:}
\quad\quad
\frac{\partial_L}{\partial_x}=\frac{\partial_L}{\partial_y}y(1-y)
```

## Affine/Softmax 层的实现

### Affine层

神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算

> :pushpin:
> 神经网络的正向传播中进行矩阵的乘积运算在几何学领域被称为“仿射变换”。因此，这里将进行仿射变换的处理实现为"Affine层"

```python
# 矩阵乘积运算中对应维度的元素个数要保持一致
y = np.dot(X, W) + B
```
```math
\frac{\partial_L}{\partial_X}=\frac{\partial_L}{\partial_Y} . W^T\\
```
```math
\frac{\partial_L}{\partial_W}= X^T . \frac{\partial_L}{\partial_Y}
```

$W^T$的T表示转置。转置操作会把W的元素(i, j)换成(j ,i)
```math
W =
 \begin{pmatrix}
  w_{11} & w_{12} &  w_{13} \\
  w_{21} & w_{22} &  w_{23}
 \end{pmatrix}
```
```math
W =
 \begin{pmatrix}
  w_{11} & w_{21}\\
  w_{12} & w_{22}\\
  w_{13} & w_{23}
 \end{pmatrix}
```

### Softmax-with-Loss 层

softmax 函数会将输入值正规化之后再输出

> :pushpin:
> 神经网络中进行的处理有**推理**和**学习**两个阶段
> 神经网络的推理通常不使用 Softmax 层
> 神经网络中未被正规化的输出结果有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax 层
> 不过，神经网络的学习阶段则需要 Softmax 层

Softmax-with-Loss 层（Softmax 函数和交叉熵误差）
softmax 函数记为 Softmax 层，交叉熵误差记为 Cross Entropy Error 层
Softmax 层将输入 $(a_1, a_2, a_3)$ 正规化，输出 $(y1, y2, y3)$
Cross Entropy Error 层接收 Softmax 的输出 $(y1, y2, y3)$ 和监督数据 $(t1, t2, t3)$，从这些数据中输出损失 L

Softmax 层的反向传播得到了 $(y1-t1, y2-t2, y3-t3)$ 这样“漂亮”的结果（Softmax 层的输出和监督数据的差分）
神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质‘

神经网络学习的目的就是通过调整权重参数，使神经网络的输出接近监督数据。因此，必须将神经网络的输出与监督标签的误差高效地传递给前面的层

## 误差反向传播法的实现

通过像组装乐高积木一样组装上一节中实现的层，可以构建神经网络

### 误差反向传播算法的梯度确认

数值微分的优点是实现简单，因此，一般情况下不太容易出错。而误差反向传播法的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确。
确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常接近）的操作称为**梯度确认**

-------

## 本章所学的内容

1. 通过使用计算图，可以直观地把握计算过程
2. 计算图的节点是由局部计算构成的。局部计算构成全局计算
3. 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以计算各个节点的导数
4. 通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传播法）
5. 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传播法的实现是否正确（梯度确认）
